{"cells":[{"cell_type":"code","source":["%%configure -f\n","{\n","    \"conf\": {\n","        \"spark.jars.packages\": \"org.apache.kafka:kafka-clients:3.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\",\n","        \"spark.serializer\": \"org.apache.spark.serializer.JavaSerializer\"\n","    }\n","\n","}\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f166a46c-3dfb-4def-b07c-e7296047a604","statement_id":-1,"statement_ids":[],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T19:13:50.6073586Z","session_start_time":"2024-06-03T19:13:50.9765092Z","execution_start_time":"2024-06-03T19:17:35.5418121Z","execution_finish_time":"2024-06-03T19:17:35.5970698Z","parent_msg_id":"a03bfdee-80d6-4e72-ae53-07eed819349a"},"text/plain":"StatementMeta(, f166a46c-3dfb-4def-b07c-e7296047a604, -1, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"e947dc69-1a2f-4691-91d6-9caf9dfb9b20"},{"cell_type":"code","source":["from pyspark.sql import functions as f\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, LongType, FloatType,DoubleType\n","\n","class kafkaConfiguration:\n","\n","    def __init__(self):\n","        self.base_data_dir = \"abfss://cf927cbd-ab77-4aa6-9ebf-d7125aace954@onelake.dfs.fabric.microsoft.com/0f844c8f-7cd3-4525-93e2-e6923cd667f6/Files/kafka_stream\"\n","        self.BOOTSTRAP_SERVERS = \"pkc-7prvp.centralindia.azure.confluent.cloud:9092\"\n","        self.protocol=\"SASL_SSL\",\n","        self.mechanism = \"PLAIN\",\n","        self.JAAS_MODULE = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n","        self.CLUSTER_API_KEY = \"ZAQ46OGMELT4IFUP\"\n","        self.CLUSTER_API_SECRETS = \"aiS9XFVEi2TZgQ6Gl6fWjoMifW6kIkIS7mkR+qXmEuXq8kGRXfK+428RUs7rFlhf\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f166a46c-3dfb-4def-b07c-e7296047a604","statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T19:22:55.638051Z","session_start_time":null,"execution_start_time":"2024-06-03T19:22:56.1410163Z","execution_finish_time":"2024-06-03T19:22:56.3962346Z","parent_msg_id":"bf1a6709-785d-4a0f-ac30-c90ebdeddc17"},"text/plain":"StatementMeta(, f166a46c-3dfb-4def-b07c-e7296047a604, 11, Finished, Available)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d5faf24e-0249-40c2-a49c-6bba89467734"},{"cell_type":"code","source":["\n","\n","class Getschema:\n","    ''' Define all schema for all the different files '''\n","\n","    # Define schema for the 'address' file\n","    def address(self):\n","        return (\n","            StructType([\n","                StructField('City', StringType(), True),\n","                StructField('Country', StringType(), True),\n","                StructField('State', StringType(), True),\n","                StructField('StreetAddress', StringType(), True),\n","                StructField('ZipCode', StringType(), True),\n","                StructField('custID', IntegerType(), True)\n","            ]) \n","        )\n","\n","    # Define schema for the 'customer' file\n","    def customer(self):\n","        return (\n","            StructType([\n","                StructField(\"Name\", StringType(), True),\n","                StructField(\"age\", IntegerType(), True),\n","                StructField(\"custID\", IntegerType(), True),\n","                StructField(\"email\", StringType(), True),\n","                StructField(\"gender\", StringType(), True),\n","                StructField(\"phoneNumber\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'cart' file\n","    def cart(self):\n","        return(\n","            StructType([\n","                StructField(\"action\", StringType()),\n","                StructField(\"CartID\", IntegerType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"Quantity\", IntegerType(), True),\n","                StructField(\"customerid\", IntegerType(), True),\n","                StructField(\"discount\", DoubleType(), True)\n","            ])\n","        )\n","    \n","    # Define schema for the 'exchange_order' file\n","    def exchange_order(self):\n","        return(\n","            StructType([\n","                StructField(\"ExchangeDate\", TimestampType(), True),\n","                StructField(\"ExchangeID\", IntegerType(), True),\n","                StructField(\"ExchangeItem\", StringType(), True),\n","                StructField(\"ExchangeReason\", StringType(), True),\n","                StructField(\"OrderID\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'inventory' file\n","    def inventory(self):\n","        return(\n","            StructType([\n","                StructField(\"LastStockUpdate\", DateType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"RestockingAlert\", StringType(), True),\n","                StructField(\"StockLevel\", IntegerType(), True),\n","                StructField(\"SupplierID\", IntegerType(), nullable=False)\n","            ])\n","        )\n","\n","    # Define schema for the 'order' file\n","    def order(self):\n","        return(StructType([\n","                StructField(\"customerID\", IntegerType()),\n","                StructField(\"OrderDate\", TimestampType()),\n","                StructField(\"OrderID\", LongType()),\n","                StructField(\"PaymentMethod\", StringType()),\n","                StructField(\"TotalAmount\", FloatType()),\n","                StructField(\"ProductID\", IntegerType())\n","            ])\n","        )\n","\n","    # Define schema for the 'return_order' file\n","    def return_order(self):\n","        return(StructType([\n","                StructField(\"OrderID\", IntegerType(), True),\n","                StructField(\"RefundAmount\", IntegerType(), True),\n","                StructField(\"ReturnDate\", TimestampType(), True),\n","                StructField(\"ReturnID\", IntegerType(), True),\n","                StructField(\"ReturnReason\", StringType(), True),\n","                StructField(\"customerid\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'product_cost' file\n","    def product_cost(self):\n","        return(StructType([\n","                StructField(\"endDate\", TimestampType(), nullable=False),\n","                StructField(\"productID\", IntegerType(), True),\n","                StructField(\"standardCost\", IntegerType(), nullable=False),\n","                StructField(\"startDate\", TimestampType(), True)\n","            ])\n","        )\n","    \n","    # Define schema for the 'product' file\n","    def product(self):\n","        return( StructType([\n","                StructField(\"Category\", StringType(), True),\n","                StructField(\"Description\", StringType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"ProductName\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'product_location' file\n","    def product_location(self):\n","        return( StructType([\n","                StructField(\"City\", StringType(), True),\n","                StructField(\"Country\", StringType(), True),\n","                StructField(\"LocationID\", IntegerType(), True),\n","                StructField(\"LocationName\", StringType(), True),\n","                StructField(\"State\", StringType(), True),\n","                StructField(\"productid\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'shipping' file\n","    def shipping(self):\n","        return( StructType([\n","                StructField(\"City\", StringType(), True),\n","                StructField(\"Country\", StringType(), True),\n","                StructField(\"DeliveryDate\", TimestampType(), True),\n","                StructField(\"ShipmentDate\", TimestampType(), True),\n","                StructField(\"ShippingAddress\", StringType(), True),\n","                StructField(\"ShippingID\", IntegerType(), True),\n","                StructField(\"State\", StringType(), True),\n","                StructField(\"ZipCode\", StringType(), True)\n","            ])\n","        )\n","    \n","    # Define schema for the 'stock_movement' file\n","    def stock_movement(self):\n","        return(StructType([\n","                StructField(\"MovementDate\", DateType(), True),\n","                StructField(\"MovementType\", StringType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"Quantity\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'supplier' file\n","    def supplier(self):\n","        return(StructType([\n","                StructField(\"ContactPerson\", StringType(), True),\n","                StructField(\"Email\", StringType(), True),\n","                StructField(\"Phone\", StringType(), True),\n","                StructField(\"SupplierID\", IntegerType(), True),\n","                StructField(\"SupplierName\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'membership' file\n","    def membership(self):\n","        return(StructType([\n","                StructField(\"End_date\", DateType(), True),\n","                StructField(\"Level\", StringType(), True),\n","                StructField(\"MembershipID\", IntegerType(), True),\n","                StructField(\"Start_date\", DateType(), True),\n","                StructField(\"custID\", IntegerType(), True)\n","            ])\n","        )\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f166a46c-3dfb-4def-b07c-e7296047a604","statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T19:21:10.7941239Z","session_start_time":null,"execution_start_time":"2024-06-03T19:21:30.4357612Z","execution_finish_time":"2024-06-03T19:21:30.7099066Z","parent_msg_id":"4707db6f-60f1-440d-b3bc-6461dbc059fb"},"text/plain":"StatementMeta(, f166a46c-3dfb-4def-b07c-e7296047a604, 6, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6363b69a-8dbb-47c2-b4d5-9a05f44c5d8c"},{"cell_type":"code","source":["\n","\n","class BronzeStream:\n","    \"\"\"\n","    - ingest data from kafka\n","    - transform value from binary to string\n","    - transform string value to struct type\n","    - save the stream to bronze table\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.conf = kafkaConfiguration()\n","        self.schema_generator = Getschema()\n","\n","    def getTopicSchema(self, topic: str) -> StructType:\n","        schema_method = getattr(self.schema_generator, topic.lower(), None)  # Get the schema method dynamically\n","        if schema_method:\n","            return schema_method()  # Call the schema method and return the schema\n","        else:\n","            raise ValueError(f\"No schema defined for topic: {topic}\")\n","\n","    def ingestFromKafka(self, topic: str):\n","        return (\n","            spark.readStream.format(\"kafka\")\n","            .option(\"kafka.bootstrap.servers\", self.conf.BOOTSTRAP_SERVERS)\n","            .option(\"kafka.security.protocol\", \"SASL_SSL\")\n","            .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n","            .option(\"kafka.sasl.jaas.config\",\n","                    f\"{self.conf.JAAS_MODULE} required username='{self.conf.CLUSTER_API_KEY}' password='{self.conf.CLUSTER_API_SECRETS}';\")\n","            .option(\"startingOffsets\", \"earliest\")\n","            .option(\"subscribe\", topic)\n","            .option(\"failOnDataLoss\", \"False\")\n","            .load()\n","        )\n","\n","    def getReading(self, kafka_df):\n","        return kafka_df.select(\n","            f.col(\"key\").cast(\"string\"), f.col(\"value\").cast(\"string\")\n","        )\n","\n","\n","class ProcessStream:\n","\n","    def __init__(self):\n","        self.bStream = BronzeStream()\n","\n","      # Handle the exception accordingly\n","\n","    def processOrder(self, topic):\n","        rawDF = self.bStream.ingestFromKafka(topic)\n","        readingDF = self.bStream.getReading(rawDF)\n","        schema = self.bStream.getTopicSchema(topic)\n","        processOrderDF = readingDF.withColumn(\n","            \"json_value\", f.from_json(f.col(\"value\"), schema)\n","        ).select(\n","            f.col(\"json_value.CustomerID\").alias(\"CustomerID\"),\n","            f.col(\"json_value.OrderDate\").alias(\"OrderDate\"),\n","            f.col(\"json_value.OrderID\").alias(\"OrderID\"),\n","            f.col(\"json_value.TotalAmount\").alias(\"TotalAmount\"),\n","            f.col(\"json_value.PaymentMethod\").alias(\"PaymentMethod\"),\n","            f.col(\"json_value.ProductID\").alias(\"ProductID\")\n","        )\n","        return processOrderDF\n","\n","    def processCart(self,topic):\n","        rawDF = self.bStream.ingestFromKafka(\"cart\")\n","        readingDF = self.bStream.getReading(rawDF)\n","        schema = self.bStream.getTopicSchema(\"cart\")\n","        processCartDF = readingDF.withColumn(\n","            \"json_value\", f.from_json(f.col(\"value\"), schema)\n","        ).select(\n","            f.col(\"json_value.CartID\").alias(\"CartID\"),\n","            f.col(\"json_value.ProductID\").alias(\"ProductID\"),\n","            f.col(\"json_value.Quantity\").alias(\"Quantity\"),\n","            f.col(\"json_value.customerid\").alias(\"customerid\"),\n","            f.col(\"json_value.discount\").alias(\"discount\"),\n","            f.col(\"json_value.action\").alias(\"action\")\n","        )\n","\n","        return processCartDF\n","\n","\n","\n","    def writeToBronzeTable(self, processDF, topic: str):\n","        table_name = f'bronze.Streaming_{topic}'\n","        checkpoint_location = f\"{self.bStream.conf.base_data_dir}/checkpoint/{topic}\"\n","        try:\n","            query = (\n","                processDF.writeStream\n","                .queryName(f\"bronze-ingestion-{topic}\")\n","                .option(\"checkpointLocation\", checkpoint_location)\n","                .outputMode(\"append\")\n","                .toTable(table_name)\n","            )\n","            return query  # Return the query object for monitoring\n","        except Exception as e:\n","            print(\"Exception caught: \", e)\n","            return None\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f166a46c-3dfb-4def-b07c-e7296047a604","statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T19:22:35.472653Z","session_start_time":null,"execution_start_time":"2024-06-03T19:22:36.0111726Z","execution_finish_time":"2024-06-03T19:22:36.266795Z","parent_msg_id":"f0aa0ff7-0423-4848-8944-842ce2863919"},"text/plain":"StatementMeta(, f166a46c-3dfb-4def-b07c-e7296047a604, 9, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3e7120ff-8d9e-4380-8afc-4f02a036e1f7","showTitle":false,"title":""},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"ca616a66-5c33-429b-aeb3-2b0d858a84fc"},{"cell_type":"code","source":["\n","# Create an instance of ProcessStream\n","processor = ProcessStream()\n","\n","# Process the Kafka stream for the \"order\" topic\n","processOrderDF = processor.processOrder(\"order\")\n","writeOrder=processor.writeToBronzeTable(processOrderDF,\"order\")\n","\n","processCartDF= processor.processCart(\"cart\")\n","writeCart=processor.writeToBronzeTable(processCartDF, \"cart\")\n","\n","# Show the schema and a few rows of the processed DataFrame\n","\n","#processOrderDF.show(5, truncate=False)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f166a46c-3dfb-4def-b07c-e7296047a604","statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T19:23:02.9675183Z","session_start_time":null,"execution_start_time":"2024-06-03T19:23:03.4834656Z","execution_finish_time":"2024-06-03T19:23:15.8581361Z","parent_msg_id":"d1f27d1d-afde-4bd9-9d8b-7ca9a01993d0"},"text/plain":"StatementMeta(, f166a46c-3dfb-4def-b07c-e7296047a604, 12, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Exception caught:  Cannot start query with name bronze-ingestion-order as a query with that name is already active in this SparkSession\n"]}],"execution_count":9,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"e5c09128-959d-4e72-b5b1-a98c3eb8802e","showTitle":false,"title":""},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"5b9bebc2-2409-4886-9fc1-bc0951911059"},{"cell_type":"code","source":["st.stop()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"76f550d2-9afa-4b96-a2eb-64f519f1ad98","statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-02T02:52:56.6775564Z","session_start_time":null,"execution_start_time":"2024-06-02T02:52:57.228075Z","execution_finish_time":"2024-06-02T02:52:57.5220936Z","parent_msg_id":"ccd9e35c-1d01-48d6-bba2-99990adff37a"},"text/plain":"StatementMeta(, 76f550d2-9afa-4b96-a2eb-64f519f1ad98, 10, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"2e7d54aa-21d4-4015-9acb-60c47d49bee8"},{"cell_type":"markdown","source":["<mark>**optimise the bronze delta Table**</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb9056f6-ede7-41d6-a10f-12b3cb638792"},{"cell_type":"markdown","source":["All the External tables are being partitioned during the bronze ingestion . so to optimise it further we will use z-ordering "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"47cda6f7-69e5-41ce-919d-2ff475f2b462"},{"cell_type":"code","source":["%%sql \n","\n","ALTER TABLE customer\n","SET TBLPROPERTIES ('delta.zOrderCols' = 'custID');\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"07dbebab-34f8-4da0-abb5-3fef9f9b8020","statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T20:49:19.1874023Z","session_start_time":null,"execution_start_time":"2024-06-03T20:49:19.7069242Z","execution_finish_time":"2024-06-03T20:49:22.1237721Z","parent_msg_id":"9a666289-9c62-4426-9711-872ddb30b219"},"text/plain":"StatementMeta(, 07dbebab-34f8-4da0-abb5-3fef9f9b8020, 21, Finished, Available)"},"metadata":{}},{"output_type":"error","ename":"Error","evalue":"Unknown configuration was specified: delta.zOrderCols\nTo disable this check, set spark.databricks.delta.allowArbitraryProperties.enabled=true in the Spark session configuration.","traceback":["Unknown configuration was specified: delta.zOrderCols\nTo disable this check, set spark.databricks.delta.allowArbitraryProperties.enabled=true in the Spark session configuration.","org.apache.spark.sql.delta.DeltaErrorsBase.unknownConfigurationKeyException(DeltaErrors.scala:702)","org.apache.spark.sql.delta.DeltaErrorsBase.unknownConfigurationKeyException$(DeltaErrors.scala:701)","org.apache.spark.sql.delta.DeltaErrors$.unknownConfigurationKeyException(DeltaErrors.scala:2808)","org.apache.spark.sql.delta.DeltaConfigsBase.$anonfun$validateConfigurations$1(DeltaConfig.scala:174)","scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)","scala.collection.immutable.Map$Map1.foreach(Map.scala:193)","scala.collection.TraversableLike.map(TraversableLike.scala:286)","scala.collection.TraversableLike.map$(TraversableLike.scala:279)","scala.collection.AbstractTraversable.map(Traversable.scala:108)","org.apache.spark.sql.delta.DeltaConfigsBase.validateConfigurations(DeltaConfig.scala:158)","org.apache.spark.sql.delta.DeltaConfigsBase.validateConfigurations$(DeltaConfig.scala:154)","org.apache.spark.sql.delta.DeltaConfigs$.validateConfigurations(DeltaConfig.scala:658)","org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$alterTable$3(DeltaCatalog.scala:570)","scala.collection.immutable.Map$Map1.foreach(Map.scala:193)","org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$alterTable$1(DeltaCatalog.scala:539)","org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)","org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)","org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)","org.apache.spark.sql.delta.catalog.DeltaCatalog.alterTable(DeltaCatalog.scala:521)","org.apache.spark.sql.delta.catalog.DeltaCatalog.alterTable(DeltaCatalog.scala:57)","org.apache.spark.sql.execution.datasources.v2.AlterTableExec.run(AlterTableExec.scala:37)","org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)","org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)","org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)","org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:152)","org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:125)","org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)","org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)","org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)","org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)","org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:152)","org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:145)","org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)","org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)","org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)","org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)","org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:145)","org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:129)","org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:123)","org.apache.spark.sql.Dataset.<init>(Dataset.scala:230)","org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)","org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)","org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)","org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)","org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)","org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)","org.apache.spark.sql.SparkSession.sql(SparkSession.scala:671)","org.apache.livy.repl.SQLInterpreter.execute(SQLInterpreter.scala:163)","org.apache.livy.repl.Session.$anonfun$executeCode$1(Session.scala:815)","scala.Option.map(Option.scala:230)","org.apache.livy.repl.Session.executeCode(Session.scala:812)","org.apache.livy.repl.Session.$anonfun$execute$9(Session.scala:538)","org.apache.livy.repl.Session.withRealtimeOutputSupport(Session.scala:1039)","org.apache.livy.repl.Session.$anonfun$execute$2(Session.scala:538)","scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)","scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","scala.util.Success.$anonfun$map$1(Try.scala:255)","scala.util.Success.map(Try.scala:213)","scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","java.base/java.lang.Thread.run(Thread.java:829)"]}],"execution_count":17,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"13bb7224-74ee-4e24-ab0f-2251225e199b"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da0e53b6-66fd-4a33-84ee-87d8adbd5a9f"}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1307964110880498,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"kafka-consumer","widgets":{}},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"0f844c8f-7cd3-4525-93e2-e6923cd667f6","default_lakehouse_name":"Bronze","default_lakehouse_workspace_id":"cf927cbd-ab77-4aa6-9ebf-d7125aace954"},"environment":{"environmentId":"f4f58294-e364-4841-bad6-da88023e50d1","workspaceId":"cf927cbd-ab77-4aa6-9ebf-d7125aace954"}}},"nbformat":4,"nbformat_minor":5}