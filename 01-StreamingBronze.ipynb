{"cells":[{"cell_type":"code","execution_count":null,"id":"e947dc69-1a2f-4691-91d6-9caf9dfb9b20","metadata":{"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":true}},"outputs":[],"source":["%%configure -f\n","{\n","    \"conf\": {\n","        \"spark.jars.packages\": \"org.apache.kafka:kafka-clients:3.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\",\n","        \"spark.serializer\": \"org.apache.spark.serializer.JavaSerializer\"\n","    }\n","\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"d5faf24e-0249-40c2-a49c-6bba89467734","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import functions as f\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, LongType, FloatType,DoubleType\n","\n","class kafkaConfiguration:\n","\n","    def __init__(self):\n","        self.base_data_dir = \n","        self.BOOTSTRAP_SERVERS = \n","        self.protocol=,\n","        self.mechanism = ,\n","        self.JAAS_MODULE = \n","        self.CLUSTER_API_KEY =\n","        self.CLUSTER_API_SECRETS = "]},{"cell_type":"code","execution_count":null,"id":"6363b69a-8dbb-47c2-b4d5-9a05f44c5d8c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["\n","\n","class Getschema:\n","    ''' Define all schema for all the different files '''\n","\n","    # Define schema for the 'address' file\n","    def address(self):\n","        return (\n","            StructType([\n","                StructField('City', StringType(), True),\n","                StructField('Country', StringType(), True),\n","                StructField('State', StringType(), True),\n","                StructField('StreetAddress', StringType(), True),\n","                StructField('ZipCode', StringType(), True),\n","                StructField('custID', IntegerType(), True)\n","            ]) \n","        )\n","\n","    # Define schema for the 'customer' file\n","    def customer(self):\n","        return (\n","            StructType([\n","                StructField(\"Name\", StringType(), True),\n","                StructField(\"age\", IntegerType(), True),\n","                StructField(\"custID\", IntegerType(), True),\n","                StructField(\"email\", StringType(), True),\n","                StructField(\"gender\", StringType(), True),\n","                StructField(\"phoneNumber\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'cart' file\n","    def cart(self):\n","        return(\n","            StructType([\n","                StructField(\"action\", StringType()),\n","                StructField(\"CartID\", IntegerType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"Quantity\", IntegerType(), True),\n","                StructField(\"customerid\", IntegerType(), True),\n","                StructField(\"discount\", DoubleType(), True)\n","            ])\n","        )\n","    \n","    # Define schema for the 'exchange_order' file\n","    def exchange_order(self):\n","        return(\n","            StructType([\n","                StructField(\"ExchangeDate\", TimestampType(), True),\n","                StructField(\"ExchangeID\", IntegerType(), True),\n","                StructField(\"ExchangeItem\", StringType(), True),\n","                StructField(\"ExchangeReason\", StringType(), True),\n","                StructField(\"OrderID\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'inventory' file\n","    def inventory(self):\n","        return(\n","            StructType([\n","                StructField(\"LastStockUpdate\", DateType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"RestockingAlert\", StringType(), True),\n","                StructField(\"StockLevel\", IntegerType(), True),\n","                StructField(\"SupplierID\", IntegerType(), nullable=False)\n","            ])\n","        )\n","\n","    # Define schema for the 'order' file\n","    def order(self):\n","        return(StructType([\n","                StructField(\"customerID\", IntegerType()),\n","                StructField(\"OrderDate\", TimestampType()),\n","                StructField(\"OrderID\", LongType()),\n","                StructField(\"PaymentMethod\", StringType()),\n","                StructField(\"TotalAmount\", FloatType()),\n","                StructField(\"ProductID\", IntegerType())\n","            ])\n","        )\n","\n","    # Define schema for the 'return_order' file\n","    def return_order(self):\n","        return(StructType([\n","                StructField(\"OrderID\", IntegerType(), True),\n","                StructField(\"RefundAmount\", IntegerType(), True),\n","                StructField(\"ReturnDate\", TimestampType(), True),\n","                StructField(\"ReturnID\", IntegerType(), True),\n","                StructField(\"ReturnReason\", StringType(), True),\n","                StructField(\"customerid\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'product_cost' file\n","    def product_cost(self):\n","        return(StructType([\n","                StructField(\"endDate\", TimestampType(), nullable=False),\n","                StructField(\"productID\", IntegerType(), True),\n","                StructField(\"standardCost\", IntegerType(), nullable=False),\n","                StructField(\"startDate\", TimestampType(), True)\n","            ])\n","        )\n","    \n","    # Define schema for the 'product' file\n","    def product(self):\n","        return( StructType([\n","                StructField(\"Category\", StringType(), True),\n","                StructField(\"Description\", StringType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"ProductName\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'product_location' file\n","    def product_location(self):\n","        return( StructType([\n","                StructField(\"City\", StringType(), True),\n","                StructField(\"Country\", StringType(), True),\n","                StructField(\"LocationID\", IntegerType(), True),\n","                StructField(\"LocationName\", StringType(), True),\n","                StructField(\"State\", StringType(), True),\n","                StructField(\"productid\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'shipping' file\n","    def shipping(self):\n","        return( StructType([\n","                StructField(\"City\", StringType(), True),\n","                StructField(\"Country\", StringType(), True),\n","                StructField(\"DeliveryDate\", TimestampType(), True),\n","                StructField(\"ShipmentDate\", TimestampType(), True),\n","                StructField(\"ShippingAddress\", StringType(), True),\n","                StructField(\"ShippingID\", IntegerType(), True),\n","                StructField(\"State\", StringType(), True),\n","                StructField(\"ZipCode\", StringType(), True)\n","            ])\n","        )\n","    \n","    # Define schema for the 'stock_movement' file\n","    def stock_movement(self):\n","        return(StructType([\n","                StructField(\"MovementDate\", DateType(), True),\n","                StructField(\"MovementType\", StringType(), True),\n","                StructField(\"ProductID\", IntegerType(), True),\n","                StructField(\"Quantity\", IntegerType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'supplier' file\n","    def supplier(self):\n","        return(StructType([\n","                StructField(\"ContactPerson\", StringType(), True),\n","                StructField(\"Email\", StringType(), True),\n","                StructField(\"Phone\", StringType(), True),\n","                StructField(\"SupplierID\", IntegerType(), True),\n","                StructField(\"SupplierName\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'membership' file\n","    def membership(self):\n","        return(StructType([\n","                StructField(\"End_date\", DateType(), True),\n","                StructField(\"Level\", StringType(), True),\n","                StructField(\"MembershipID\", IntegerType(), True),\n","                StructField(\"Start_date\", DateType(), True),\n","                StructField(\"custID\", IntegerType(), True)\n","            ])\n","        )\n"]},{"cell_type":"code","execution_count":null,"id":"ca616a66-5c33-429b-aeb3-2b0d858a84fc","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3e7120ff-8d9e-4380-8afc-4f02a036e1f7","showTitle":false,"title":""},"editable":true,"microsoft":{"language":"python","language_group":"synapse_pyspark"},"run_control":{"frozen":false}},"outputs":[],"source":["\n","\n","class BronzeStream:\n","    \"\"\"\n","    - ingest data from kafka\n","    - transform value from binary to string\n","    - transform string value to struct type\n","    - save the stream to bronze table\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.conf = kafkaConfiguration()\n","        self.schema_generator = Getschema()\n","\n","    def getTopicSchema(self, topic: str) -> StructType:\n","        schema_method = getattr(self.schema_generator, topic.lower(), None)  # Get the schema method dynamically\n","        if schema_method:\n","            return schema_method()  # Call the schema method and return the schema\n","        else:\n","            raise ValueError(f\"No schema defined for topic: {topic}\")\n","\n","    def ingestFromKafka(self, topic: str):\n","        return (\n","            spark.readStream.format(\"kafka\")\n","            .option(\"kafka.bootstrap.servers\", self.conf.BOOTSTRAP_SERVERS)\n","            .option(\"kafka.security.protocol\", \"SASL_SSL\")\n","            .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n","            .option(\"kafka.sasl.jaas.config\",\n","                    f\"{self.conf.JAAS_MODULE} required username='{self.conf.CLUSTER_API_KEY}' password='{self.conf.CLUSTER_API_SECRETS}';\")\n","            .option(\"startingOffsets\", \"earliest\")\n","            .option(\"subscribe\", topic)\n","            .option(\"failOnDataLoss\", \"False\")\n","            .load()\n","        )\n","\n","    def getReading(self, kafka_df):\n","        return kafka_df.select(\n","            f.col(\"key\").cast(\"string\"), f.col(\"value\").cast(\"string\")\n","        )\n","\n","\n","class ProcessStream:\n","\n","    def __init__(self):\n","        self.bStream = BronzeStream()\n","\n","      # Handle the exception accordingly\n","\n","    def processOrder(self, topic):\n","        rawDF = self.bStream.ingestFromKafka(topic)\n","        readingDF = self.bStream.getReading(rawDF)\n","        schema = self.bStream.getTopicSchema(topic)\n","        processOrderDF = readingDF.withColumn(\n","            \"json_value\", f.from_json(f.col(\"value\"), schema)\n","        ).select(\n","            f.col(\"json_value.CustomerID\").alias(\"CustomerID\"),\n","            f.col(\"json_value.OrderDate\").alias(\"OrderDate\"),\n","            f.col(\"json_value.OrderID\").alias(\"OrderID\"),\n","            f.col(\"json_value.TotalAmount\").alias(\"TotalAmount\"),\n","            f.col(\"json_value.PaymentMethod\").alias(\"PaymentMethod\"),\n","            f.col(\"json_value.ProductID\").alias(\"ProductID\")\n","        )\n","        return processOrderDF\n","\n","    def processCart(self,topic):\n","        rawDF = self.bStream.ingestFromKafka(\"cart\")\n","        readingDF = self.bStream.getReading(rawDF)\n","        schema = self.bStream.getTopicSchema(\"cart\")\n","        processCartDF = readingDF.withColumn(\n","            \"json_value\", f.from_json(f.col(\"value\"), schema)\n","        ).select(\n","            f.col(\"json_value.CartID\").alias(\"CartID\"),\n","            f.col(\"json_value.ProductID\").alias(\"ProductID\"),\n","            f.col(\"json_value.Quantity\").alias(\"Quantity\"),\n","            f.col(\"json_value.customerid\").alias(\"customerid\"),\n","            f.col(\"json_value.discount\").alias(\"discount\"),\n","            f.col(\"json_value.action\").alias(\"action\")\n","        )\n","\n","        return processCartDF\n","\n","\n","\n","    def writeToBronzeTable(self, processDF, topic: str):\n","        table_name = f'bronze.Streaming_{topic}'\n","        checkpoint_location = f\"{self.bStream.conf.base_data_dir}/checkpoint/{topic}\"\n","        try:\n","            query = (\n","                processDF.writeStream\n","                .queryName(f\"bronze-ingestion-{topic}\")\n","                .option(\"checkpointLocation\", checkpoint_location)\n","                .outputMode(\"append\")\n","                .toTable(table_name)\n","            )\n","            return query  # Return the query object for monitoring\n","        except Exception as e:\n","            print(\"Exception caught: \", e)\n","            return None\n"]},{"cell_type":"code","execution_count":null,"id":"5b9bebc2-2409-4886-9fc1-bc0951911059","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"e5c09128-959d-4e72-b5b1-a98c3eb8802e","showTitle":false,"title":""},"editable":true,"microsoft":{"language":"python","language_group":"synapse_pyspark"},"run_control":{"frozen":false}},"outputs":[],"source":["\n","# Create an instance of ProcessStream\n","processor = ProcessStream()\n","\n","# Process the Kafka stream for the \"order\" topic\n","processOrderDF = processor.processOrder(\"order\")\n","writeOrder=processor.writeToBronzeTable(processOrderDF,\"order\")\n","\n","processCartDF= processor.processCart(\"cart\")\n","writeCart=processor.writeToBronzeTable(processCartDF, \"cart\")\n","\n","# Show the schema and a few rows of the processed DataFrame\n","\n","#processOrderDF.show(5, truncate=False)\n","\n"]},{"cell_type":"markdown","id":"eb9056f6-ede7-41d6-a10f-12b3cb638792","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["<mark>**optimise the bronze delta Table**</mark>"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1307964110880498,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"kafka-consumer","widgets":{}},"dependencies":{"environment":{"environmentId":"f4f58294-e364-4841-bad6-da88023e50d1","workspaceId":"cf927cbd-ab77-4aa6-9ebf-d7125aace954"},"lakehouse":{"default_lakehouse":"0f844c8f-7cd3-4525-93e2-e6923cd667f6","default_lakehouse_name":"Bronze","default_lakehouse_workspace_id":"cf927cbd-ab77-4aa6-9ebf-d7125aace954"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
