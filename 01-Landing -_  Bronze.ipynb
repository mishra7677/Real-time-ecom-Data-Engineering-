{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import StructField , StructType,StringType, IntegerType, LongType,DoubleType,FloatType,TimestampType,DateType\n","import datetime\n","from pyspark.sql import functions as f\n","from pyspark.sql.functions import current_timestamp\n","from notebookutils import mssparkutils"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ef24dae4-5fdc-491c-8a83-28b2c718b671","statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T06:21:29.6498499Z","session_start_time":null,"execution_start_time":"2024-06-03T06:21:31.1153516Z","execution_finish_time":"2024-06-03T06:21:31.3809236Z","parent_msg_id":"0e5b8ade-474c-4029-9d17-418cd624da7d"},"text/plain":"StatementMeta(, ef24dae4-5fdc-491c-8a83-28b2c718b671, 14, Finished, Available)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3514cf2-7942-4560-8aca-a2b8ef20edd8"},{"cell_type":"code","source":["schema_name=\"abc\"\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"731a8775-fe2b-4a46-bb46-cc3dfbb8a425"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e7bb5a6b-bb02-4555-b36f-27f53495ebd0"},{"cell_type":"code","source":["class Getschema:\n","    ''' Define all schema for all the different files '''\n","\n","    # Define schema for the 'address' file\n","    def address(self):\n","        return (\n","            StructType([\n","                StructField('City', StringType()),\n","                StructField('Country', StringType()),\n","                StructField('State', StringType()),\n","                StructField('StreetAddress', StringType()),\n","                StructField('ZipCode', StringType()),\n","                StructField('custID', LongType())\n","            ]) \n","        )\n","\n","    # Define schema for the 'customer' file\n","    def customer(self):\n","        return (\n","            StructType([\n","                StructField(\"Name\", StringType()),\n","                StructField(\"age\", LongType()),\n","                StructField(\"custID\", LongType()),\n","                StructField(\"email\", StringType()),\n","                StructField(\"gender\", StringType()),\n","                StructField(\"phoneNumber\", StringType())\n","            ])\n","        )\n","\n","    # Define schema for the 'cart' file\n","    def cart(self):\n","        return(\n","            StructType([\n","                StructField(\"CartID\", LongType()),\n","                StructField(\"ProductID\", LongType()),\n","                StructField(\"Quantity\", LongType()),\n","                StructField(\"customerid\", LongType()),\n","                StructField(\"discount\", DoubleType())\n","            ])\n","        )\n","    \n","    # Define schema for the 'exchange_order' file\n","    def exchange_order(self):\n","        return(\n","            StructType([\n","                StructField(\"ExchangeDate\", TimestampType()),\n","                StructField(\"ExchangeID\", LongType()),\n","                StructField(\"ExchangeItem\", StringType()),\n","                StructField(\"ExchangeReason\", StringType()),\n","                StructField(\"OrderID\", LongType())\n","            ])\n","        )\n","\n","    # Define schema for the 'inventory' file\n","    def inventory(self):\n","        return(\n","            StructType([\n","                StructField(\"LastStockUpdate\", DateType()),\n","                StructField(\"ProductID\", LongType()),\n","                StructField(\"RestockingAlert\", StringType()),\n","                StructField(\"StockLevel\", LongType()),\n","                StructField(\"SupplierID\", LongType(), nullable=False)\n","            ])\n","        )\n","\n","    # Define schema for the 'order' file\n","    def order(self):\n","        return(StructType([\n","                StructField(\"CustomerID\", LongType(), nullable=False),\n","                StructField(\"OrderDate\", TimestampType()),\n","                StructField(\"OrderID\", LongType(), nullable=False),\n","                StructField(\"PaymentMethod\", StringType(), nullable=False),\n","                StructField(\"TotalAmount\", LongType()),\n","                StructField(\"productID\", LongType(), nullable=False)\n","            ])\n","        )\n","\n","    # Define schema for the 'return_order' file\n","    def return_order(self):\n","        return(StructType([\n","                StructField(\"OrderID\", LongType(), True),\n","                StructField(\"RefundAmount\", LongType(), True),\n","                StructField(\"ReturnDate\", TimestampType(), True),\n","                StructField(\"ReturnID\", LongType(), True),\n","                StructField(\"ReturnReason\", StringType(), True),\n","                StructField(\"customerid\", LongType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'product_cost' file\n","    def product_cost(self):\n","        return(StructType([\n","                StructField(\"endDate\", TimestampType(), nullable=False),\n","                StructField(\"productID\", LongType()),\n","                StructField(\"standardCost\", LongType(), nullable=False),\n","                StructField(\"startDate\", TimestampType())\n","            ])\n","        )\n","    \n","    # Define schema for the 'product' file\n","    def product(self):\n","        return( StructType([\n","                StructField(\"Category\", StringType(), True),\n","                StructField(\"Description\", StringType(), True),\n","                StructField(\"ProductID\", LongType(), True),\n","                StructField(\"ProductName\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'product_location' file\n","    def product_location(self):\n","        return( StructType([\n","                StructField(\"City\", StringType(), True),\n","                StructField(\"Country\", StringType(), True),\n","                StructField(\"LocationID\", LongType(), True),\n","                StructField(\"LocationName\", StringType(), True),\n","                StructField(\"State\", StringType(), True),\n","                StructField(\"productid\", LongType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'shipping' file\n","    def shipping(self):\n","        return( StructType([\n","                StructField(\"City\", StringType(), True),\n","                StructField(\"Country\", StringType(), True),\n","                StructField(\"DeliveryDate\", TimestampType(), True),\n","                StructField(\"ShipmentDate\", TimestampType(), True),\n","                StructField(\"ShippingAddress\", StringType(), True),\n","                StructField(\"ShippingID\", LongType(), True),\n","                StructField(\"State\", StringType(), True),\n","                StructField(\"ZipCode\", StringType(), True)\n","            ])\n","        )\n","    \n","    # Define schema for the 'stock_movement' file\n","    def stock_movement(self):\n","        return(StructType([\n","                StructField(\"MovementDate\", DateType(), True),\n","                StructField(\"MovementType\", StringType(), True),\n","                StructField(\"ProductID\", LongType(), True),\n","                StructField(\"Quantity\", LongType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'supplier' file\n","    def supplier(self):\n","        return(StructType([\n","                StructField(\"ContactPerson\", StringType(), True),\n","                StructField(\"Email\", StringType(), True),\n","                StructField(\"Phone\", StringType(), True),\n","                StructField(\"SupplierID\", LongType(), True),\n","                StructField(\"SupplierName\", StringType(), True)\n","            ])\n","        )\n","\n","    # Define schema for the 'membership' file\n","    def membership(self):\n","        return(StructType([\n","                StructField(\"End_date\", DateType(), True),\n","                StructField(\"Level\", StringType(), True),\n","                StructField(\"MembershipID\", LongType(), True),\n","                StructField(\"Start_date\", DateType(), True),\n","                StructField(\"custID\", LongType(), True)\n","            ])\n","        )\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ef24dae4-5fdc-491c-8a83-28b2c718b671","statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T06:21:29.7049596Z","session_start_time":null,"execution_start_time":"2024-06-03T06:21:31.8231659Z","execution_finish_time":"2024-06-03T06:21:32.1003612Z","parent_msg_id":"209310df-8356-40f4-aa6a-dc4bc625626e"},"text/plain":"StatementMeta(, ef24dae4-5fdc-491c-8a83-28b2c718b671, 15, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ab157ef6-c6bd-4f92-bd1a-4765ce9062cb"},{"cell_type":"code","source":["class Transformation:\n","\n","    def address(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'address' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing address data.\n","\n","        Steps:\n","        1. Casts 'custID' and 'ZipCode' columns to integers.\n","        2. Filters out rows with null 'custID'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return(dataframe.withColumn('custID', f.col('custID').cast(\"integer\"))\n","                      .withColumn('ZipCode', f.col('ZipCode').cast('integer')) \n","                      .filter(f.col('custID').isNotNull())\n","                )\n","\n","\n","    def cart(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'cart' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing cart data.\n","\n","        Steps:\n","        1. Casts specified columns to integers.\n","        2. Casts 'discount' column to float.\n","        3. Filters out rows with null values in critical columns.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        for field in dataframe.columns[:4]:\n","            dataframe = dataframe.withColumn(field, f.col(field).cast('integer'))\n","        df = dataframe.withColumn('discount', f.col('discount').cast('float'))\\\n","                   .filter(f.col('CartID').isNotNull())\\\n","                   .filter(f.col('ProductID').isNotNull())\\\n","                   .filter(f.col('customerid').isNotNull())\n","        return df\n","\n","    def customer(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'customer' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing customer data.\n","\n","        Steps:\n","        1. Casts 'custID' and 'age' columns to integers.\n","        2. Filters out rows with null 'custID'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('custID', f.col('custID').cast('integer')) \n","                        .withColumn('age', f.col('age').cast('integer')) \n","                        .filter(f.col('custID').isNotNull())\n","            )\n","\n","    def order(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'order' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing order data.\n","\n","        Steps:\n","        1. Casts specified columns to integers.\n","        2. Filters out rows with null values in critical columns.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('CustomerID', f.col('CustomerID').cast('integer')) \n","                        .withColumn('TotalAmount', f.col('TotalAmount').cast('integer')) \n","                        .withColumn('ProductID', f.col('ProductID').cast('integer')) \n","                        .filter(f.col('CustomerID').isNotNull()) \n","                        .filter(f.col('OrderID').isNotNull())\n","            )\n","\n","    def return_order(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'return_order' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing return order data.\n","\n","        Steps:\n","        1. Casts specified columns to appropriate data types.\n","        2. Filters out rows with null values in critical columns.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('OrderID', f.col('OrderID').cast('integer')) \n","                        .withColumn('RefundAmount', f.col('RefundAmount').cast('float')) \n","                        .withColumn('ReturnID', f.col('ReturnID').cast('integer')) \n","                        .withColumn('customerid', f.col('customerid').cast('integer')) \n","                        .filter(f.col('CustomerID').isNotNull()) \n","                        .filter(f.col('OrderID').isNotNull()) \n","                        .filter(f.col('ReturnID').isNotNull())\n","            )\n","\n","    def exchange_order(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'exchange_order' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing exchange order data.\n","\n","        Steps:\n","        1. Casts specified columns to integers.\n","        2. Filters out rows with null values in critical columns.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('ExchangeID', f.col('ExchangeID').cast('integer')) \n","                        .withColumn('OrderID', f.col('OrderID').cast('integer')) \n","                        .filter(f.col('ExchangeID').isNotNull()) \n","                        .filter(f.col('OrderID').isNotNull())\n","            )\n","\n","    def inventory(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'inventory' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing inventory data.\n","\n","        Steps:\n","        1. Casts specified columns to integers.\n","        2. Filters out rows with null values in critical columns.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('ProductID', f.col('ProductID').cast('integer'))\n","                     .withColumn('StockLevel', f.col('StockLevel').cast('integer'))\n","                     .withColumn('SupplierID', f.col('SupplierID').cast('integer'))\n","                     .filter(f.col('ProductID').isNotNull())\n","                     .filter(f.col('SupplierID').isNotNull())\n","        )\n","\n","\n","    def product(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'product' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing product data.\n","\n","        Steps:\n","        1. Casts 'ProductID' to integer.\n","        2. Filters out rows with null 'ProductID'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('ProductID', f.col('ProductID').cast('integer')) \n","                        .filter(f.col('ProductID').isNotNull())\n","            )\n","\n","    def product_cost(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'product_cost' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing product cost data.\n","\n","        Steps:\n","        1. Casts 'ProductID' to integer, renames it to 'Product_ID', and drops the original 'ProductID' column.\n","        2. Filters out rows with null 'Product_ID'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('Product_ID', f.col('ProductID').cast('integer')) \n","                        .drop(f.col('ProductID')) \n","                        .filter(f.col('Product_ID').isNotNull())\n","            )\n","\n","    def product_location(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'product_location' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing product location data.\n","\n","        Steps:\n","        1. Casts 'LocationID' and 'productid' to integers.\n","        2. Filters out rows with null 'LocationID' and 'productid'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('LocationID', f.col('LocationID').cast('integer')) \n","                        .withColumn('productid', f.col('productid').cast('integer')) \n","                        .filter(f.col('productid').isNotNull()) \n","                        .filter(f.col('LocationID').isNotNull())\n","            )\n","\n","    def shipping(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'shipping' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing shipping data.\n","\n","        Steps:\n","        1. Casts 'ShippingID' and 'ZipCode' to integers.\n","        2. Filters out rows with null 'ShippingID' and 'ZipCode'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('ShippingID', f.col('ShippingID').cast('integer')) \n","                        .withColumn('ZipCode', f.col('ZipCode').cast('integer')) \n","                        .filter(f.col('ShippingID').isNotNull()) \n","                        .filter(f.col('ZipCode').isNotNull())\n","            )\n","\n","    def stock_movement(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'stock_movement' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing stock movement data.\n","\n","        Steps:\n","        1. Casts 'ProductID' and 'Quantity' to integers.\n","        2. Filters out rows with null 'ProductID' and 'Quantity'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('ProductID', f.col('ProductID').cast('integer')) \n","                        .withColumn('Quantity', f.col('Quantity').cast('integer')) \n","                        .filter(f.col('ProductID').isNotNull()) \n","                        .filter(f.col('Quantity').isNotNull())\n","            )\n","\n","    def supplier(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'supplier' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing supplier data.\n","\n","        Steps:\n","        1. Casts 'SupplierID' to integer.\n","        2. Filters out rows with null 'SupplierID'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('SupplierID', f.col('SupplierID').cast('integer')) \n","                        .filter(f.col('SupplierID').isNotNull())\n","\n","            )\n","    def membership(self, dataframe):\n","        \"\"\"\n","        Cleans and transforms the 'membership' DataFrame.\n","        \n","        Parameters:\n","        - dataframe: The input DataFrame containing membership data.\n","\n","        Steps:\n","        1. Casts 'MembershipID' and 'custID' to integers.\n","        2. Filters out rows with null 'MembershipID' and 'custID'.\n","\n","        Returns:\n","        - Transformed DataFrame.\n","        \"\"\"\n","        return (\n","            dataframe.withColumn('MembershipID', f.col('MembershipID').cast('integer')) \n","                        .withColumn('custID', f.col('custID').cast('integer')) \n","                        .filter(f.col('MembershipID').isNotNull()) \n","                        .filter(f.col('custID').isNotNull())\n","            )\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ef24dae4-5fdc-491c-8a83-28b2c718b671","statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T06:21:29.7707465Z","session_start_time":null,"execution_start_time":"2024-06-03T06:21:32.5327472Z","execution_finish_time":"2024-06-03T06:21:32.7760894Z","parent_msg_id":"d9bd90f5-70c6-4ead-9ece-002ef4477c3e"},"text/plain":"StatementMeta(, ef24dae4-5fdc-491c-8a83-28b2c718b671, 16, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"63c6aaa0-1d3b-4692-9bcc-861e21561581"},{"cell_type":"code","source":["class Bronze:\n","    def __init__(self):\n","        # Initialize the base directory for data loading\n","        self.base_dir = 'abfss://cf927cbd-ab77-4aa6-9ebf-d7125aace954@onelake.dfs.fabric.microsoft.com/0f844c8f-7cd3-4525-93e2-e6923cd667f6/Files/landing/'\n","\n","    def getSchema(self, schemaName):\n","        \"\"\"\n","        Get schema for a specified schema name.\n","\n","        Parameters:\n","        - schemaName: Name of the schema to retrieve.\n","\n","        Returns:\n","        - The schema object.\n","        \"\"\"\n","        sch = Getschema()  # Assuming Getschema is a class that defines schemas\n","        schema_method = getattr(sch, schemaName)\n","        return schema_method()\n","\n","    def validateScehma(self, schemaName):\n","        \"\"\"\n","        Validate if the actual columns in the DataFrame match the expected schema columns.\n","\n","        Parameters:\n","        - schemaName: Name of the schema to validate.\n","\n","        Returns:\n","        - True if schema matches, False otherwise.\n","        \"\"\"\n","        try:\n","            # Load an empty DataFrame with the specified schema\n","            df = spark.read.format('parquet').load(f\"{self.base_dir}{schemaName}/\").limit(0)\n","\n","            # Get actual and expected column sets\n","            actual_cols = set(df.columns)\n","            expected_cols = set(self.getSchema(f\"{schemaName}\").fieldNames())\n","\n","            if actual_cols == expected_cols:\n","                return True\n","            else:\n","                # Get mismatched columns and create an insert query for rejection\n","                mismatched_columns = ', '.join(actual_cols - expected_cols)\n","                insertQuery = f\"\"\"\n","                                INSERT INTO rejectionTable \n","                                VALUES (\n","                                    current_timestamp(),\n","                                    '{schemaName}',\n","                                    '{self.base_dir}{schemaName}/',\n","                                    '{mismatched_columns}'\n","                                ) \"\"\"\n","                spark.sql(insertQuery)\n","        except Exception as e:\n","            print(f\"error loading the file : {str(e)}\")\n","            return False\n","\n","    def loadData(self, schema_name):\n","        \"\"\"\n","        Load data for a specified schema name.\n","\n","        Parameters:\n","        - schema_name: Name of the schema to load data for.\n","\n","        Returns:\n","        - DataFrame containing the loaded data.\n","        \"\"\"\n","        return spark.read.format(\"parquet\").schema(self.getSchema(schema_name)).load(f\"{self.base_dir}{schema_name}/\")\n","\n","    def process(self, schema_Name):\n","        \"\"\"\n","        Process data for a specified schema name.\n","\n","        Parameters:\n","        - schema_Name: Name of the schema to process.\n","\n","        Returns:\n","        - Transformed DataFrame after processing.\n","        \"\"\"\n","        # Validate the schema before processing\n","        if self.validateScehma(schema_Name):\n","            # Load data for the schema\n","            df = self.loadData(schema_Name)\n","            # Initialize Transformation class\n","            trans = Transformation()\n","            transform_method = getattr(trans, schema_Name)\n","            # Apply transformation and return the transformed DataFrame\n","            df_transformed = transform_method(df)\n","            return df_transformed\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ef24dae4-5fdc-491c-8a83-28b2c718b671","statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T06:21:29.7846832Z","session_start_time":null,"execution_start_time":"2024-06-03T06:21:33.213599Z","execution_finish_time":"2024-06-03T06:21:33.4667005Z","parent_msg_id":"9f557b4a-9075-4dbd-80f3-7d27a86019f0"},"text/plain":"StatementMeta(, ef24dae4-5fdc-491c-8a83-28b2c718b671, 17, Finished, Available)"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"7ef7330e-f10a-4403-b82e-c837bca78951"},{"cell_type":"code","source":["class save:\n","    def __init__(self):\n","        self.base_write_dir = 'abfss://cf927cbd-ab77-4aa6-9ebf-d7125aace954@onelake.dfs.fabric.microsoft.com/0f844c8f-7cd3-4525-93e2-e6923cd667f6/Files/bronzeDelta/'\n","        self.partition_dict = {\n","            \"address\": {\"partition_columns\": [\"custID\"], \"num_partitions\": 4},\n","            \"cart\": {\"partition_columns\": [\"customerid\"], \"num_partitions\": 4},\n","            \"customer\": {\"partition_columns\": [\"custID\"], \"num_partitions\": 4},\n","            \"order\": {\"partition_columns\": [\"CustomerID\"], \"num_partitions\": 4},\n","            \"return_order\": {\"partition_columns\": [\"OrderID\"], \"num_partitions\": 4},\n","            \"exchange_order\": {\"partition_columns\": [\"OrderID\"], \"num_partitions\": 4},\n","            \"inventory\": {\"partition_columns\": [\"ProductID\"], \"num_partitions\": 4},\n","            \"product\": {\"partition_columns\": [\"ProductID\"], \"num_partitions\": 4},\n","            \"product_cost\": {\"partition_columns\": [\"Product_ID\"], \"num_partitions\": 4},\n","            \"product_location\": {\"partition_columns\": [\"country\", \"productid\"], \"num_partitions\": 4},\n","            \"shipping\": {\"partition_columns\": [\"ShippingID\"], \"num_partitions\": 4},\n","            \"stock_movement\": {\"partition_columns\": [\"ProductID\"], \"num_partitions\": 4},\n","            \"supplier\": {\"partition_columns\": [\"SupplierID\"], \"num_partitions\": 4},\n","            \"membership\": {\"partition_columns\": [\"custID\"], \"num_partitions\": 4}\n","        }\n","\n","    def checkTableAvailability(self, schema_name):\n","        tables = spark.catalog.listTables()\n","        for table in tables:\n","            if schema_name == table.name:\n","                return True\n","                \n","                \n","        \n","        \n","\n","    def save_to_Bronze(self, df, schema_name):\n","        partition_info = self.partition_dict.get(schema_name)\n","        if partition_info:\n","            num_partition = partition_info['num_partitions']\n","            col_partition = partition_info['partition_columns']\n","            df2 = df.repartition(num_partition, *col_partition)\n","        else:\n","            df2 = df\n","\n","        # Save DataFrame to silver location\n","        path = f\"{self.base_write_dir}{schema_name}/\"\n","        df2.write.format(\"delta\").mode(\"overwrite\").save(path)\n","\n","        if not self.checkTableAvailability(schema_name):\n","            query = f\"\"\"\n","            CREATE TABLE {schema_name} \n","            USING DELTA \n","            LOCATION '{path}'\n","            \"\"\"\n","            spark.sql(query)\n","\n","        return df2\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ef24dae4-5fdc-491c-8a83-28b2c718b671","statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T06:21:29.8554496Z","session_start_time":null,"execution_start_time":"2024-06-03T06:21:33.9129711Z","execution_finish_time":"2024-06-03T06:21:34.1817334Z","parent_msg_id":"6f9c79d5-8848-49cc-b7a2-6b923c0da96c"},"text/plain":"StatementMeta(, ef24dae4-5fdc-491c-8a83-28b2c718b671, 18, Finished, Available)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"50e0ad77-117f-4526-bd45-bfc4a83c8c39"},{"cell_type":"code","source":["# Retrieve the parameter value\n","\n","bQuery= Bronze()\n","processQuery= bQuery.process(schema_name)\n","\n","sQuery= save()\n","\n","sQuery.save_to_Bronze(processQuery, schema_name)\n","mssparkutils.notebook.exit(f\"proccessing done for {schema_name}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ef24dae4-5fdc-491c-8a83-28b2c718b671","statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","queued_time":"2024-06-03T06:23:45.1699178Z","session_start_time":null,"execution_start_time":"2024-06-03T06:23:45.6583863Z","execution_finish_time":"2024-06-03T06:23:45.9321317Z","parent_msg_id":"5c9ac794-a310-46dd-9317-02b376b0eff0"},"text/plain":"StatementMeta(, ef24dae4-5fdc-491c-8a83-28b2c718b671, 22, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["error loading the file : [PATH_NOT_FOUND] Path does not exist: abfss://cf927cbd-ab77-4aa6-9ebf-d7125aace954@onelake.dfs.fabric.microsoft.com/0f844c8f-7cd3-4525-93e2-e6923cd667f6/Files/landing/abc.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'NoneType' object has no attribute 'write'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[58], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m processQuery\u001b[38;5;241m=\u001b[39m bQuery\u001b[38;5;241m.\u001b[39mprocess(schema_name)\n\u001b[1;32m      7\u001b[0m sQuery\u001b[38;5;241m=\u001b[39m save()\n\u001b[0;32m----> 9\u001b[0m \u001b[43msQuery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_Bronze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m mssparkutils\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproccessing done for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[46], line 42\u001b[0m, in \u001b[0;36msave.save_to_Bronze\u001b[0;34m(self, df, schema_name)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Save DataFrame to silver location\u001b[39;00m\n\u001b[1;32m     41\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_write_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mschema_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(path)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckTableAvailability(schema_name):\n\u001b[1;32m     45\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124m    CREATE TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m    USING DELTA \u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m    LOCATION \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'write'"]}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":[]},"id":"e888f13d-39de-4898-a86b-e4c32927783b"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ed921fb1-e986-4646-9dce-4edd7b20f9d9"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c49ae804-c6d3-42fd-8018-bf883e343d01"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"0f844c8f-7cd3-4525-93e2-e6923cd667f6"},{"id":"7432d6ed-1dd7-41f0-8b94-3ce6ca527366"},{"id":"091949f0-2cef-4ae0-9dce-32f3ec17d0c1"}],"default_lakehouse":"0f844c8f-7cd3-4525-93e2-e6923cd667f6","default_lakehouse_name":"Bronze","default_lakehouse_workspace_id":"cf927cbd-ab77-4aa6-9ebf-d7125aace954"},"environment":{}}},"nbformat":4,"nbformat_minor":5}